model_list:
  - model_name: local-generate
    litellm_params:
      custom_llm_provider: openai
      api_base: "http://127.0.0.1:8000/v1"
      api_key: "sk-no-key-needed"
      model: "Qwen/Qwen2.5-7B-Instruct"

  # - model_name: local-embed
  #   litellm_params:
  #     custom_llm_provider: ollama
  #     api_base: "http://127.0.0.1:11434"   # using the SSH tunnel
  #     model: "nomic-embed-text"
  #     timeout: 30
  #     extra_headers:
  #       Accept: "application/json"
  - model_name: local-embed
    litellm_params:
      custom_llm_provider: ollama
      api_base: "http://127.0.0.1:11434"
      model: "nomic-embed-text"

  - model_name: local-code
    litellm_params:
      custom_llm_provider: ollama
      api_base: "http://127.0.0.1:11434"  # or your tunnel/remote
      model: "deepseek-coder"